{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  6.2 中文文本处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018', '年', '世界杯', '小组赛', '抽签', '在', '莫斯科', '克里姆林宫', '举行']\n",
      "['2018', '年', '世界', '世界杯', '小组', '小组赛', '抽签', '签在', '莫斯科', '科克', '克里', '克里姆', '克里姆林', '克里姆林宫', '里姆', '里姆林', '姆林宫', '举行']\n",
      "['2018', '年', '世界', '世界杯', '小组', '小组赛', '抽签', '在', '莫斯科', '克里', '里姆', '克里姆', '里姆林', '姆林宫', '克里姆林宫', '举行']\n"
     ]
    }
   ],
   "source": [
    "#例子6-1：将文本句子 “2018年世界杯小组赛抽签在莫斯科克里姆林宫举行”进行分词。\n",
    "\n",
    "import jieba\n",
    "print( jieba.lcut(\"2018年世界杯小组赛抽签在莫斯科克里姆林宫举行\") )\n",
    "print( jieba.lcut(\"2018年世界杯小组赛抽签在莫斯科克里姆林宫举行\", cut_all = True) )\n",
    "print( jieba.lcut_for_search(\"2018年世界杯小组赛抽签在莫斯科克里姆林宫举行\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:2018, tag:m\n",
      "word:年, tag:m\n",
      "word:世界杯, tag:nz\n",
      "word:小组赛, tag:n\n",
      "word:抽签, tag:v\n",
      "word:在, tag:p\n",
      "word:莫斯科, tag:nr\n",
      "word:克里姆林宫, tag:nrt\n",
      "word:举行, tag:v\n"
     ]
    }
   ],
   "source": [
    "# 例6-2：显示例6-1分词后每个词的词性。\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"2018年世界杯小组赛抽签在莫斯科克里姆林宫举行\")\n",
    "for word, tag in words:\n",
    "    print('word:{}, tag:{}'.format(word, tag) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 是 中国 人 ， 我 爱 中国', '我 是 上海 人', '我 住 在 上海 松江 大学城']\n"
     ]
    }
   ],
   "source": [
    "#例6-3：提取文档集的词袋特征\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "#给出文档集，放在字符串列表中\n",
    "corpus = [\"我是中国人，我爱中国\", \"我是上海人\", \"我住在上海松江大学城\" ]\n",
    "split_corpus = [] #初始化存储jieba分词后的列表\n",
    "#循环为corpus中的每个字符串分词\n",
    "for c in corpus:\n",
    "    #将jieba分词后的字符串列表拼接为一个字符串，元素之间用\" \"分割\n",
    "    s = \" \".join(jieba.lcut(c)) #将分词得到的列表\n",
    "    split_corpus.append(s) #将分词结果字符串添加到列表中\n",
    "print(split_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海', '中国', '大学城', '松江']\n",
      "[[0 2 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#生成词袋\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(split_corpus)\n",
    "print(cv.get_feature_names())  #显示特征列表\n",
    "print(cv_fit.toarray())  #显示特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海', '中国', '人', '住', '在', '大学城', '我', '是', '松江', '爱']\n",
      "[[0 2 1 0 0 0 2 1 0 1]\n",
      " [1 0 1 0 0 0 1 1 0 0]\n",
      " [1 0 0 1 1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#修改token_pattern默认参数,保留所有词特征\n",
    "cv = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "cv_fit=cv.fit_transform(split_corpus)\n",
    "print(cv.get_feature_names())  #显示特征列表\n",
    "print(cv_fit.toarray())  #显示特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.72777291 0.27674503 0.         0.         0.\n",
      "  0.42983441 0.27674503 0.         0.36388646]\n",
      " [0.52682017 0.         0.52682017 0.         0.         0.\n",
      "  0.40912286 0.52682017 0.         0.        ]\n",
      " [0.34261996 0.         0.         0.45050407 0.45050407 0.45050407\n",
      "  0.26607496 0.         0.45050407 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#例6-4：提取TF-IDF模型特征。\n",
    "\n",
    "#第一种方法：将词袋特征转化为TF-IDF特征\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_fit = tfidf_transformer.fit_transform(cv_fit)\n",
    "#显示TF-IDF特征向量\n",
    "print(tfidf_fit.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.72777291 0.27674503 0.         0.         0.\n",
      "  0.42983441 0.27674503 0.         0.36388646]\n",
      " [0.52682017 0.         0.52682017 0.         0.         0.\n",
      "  0.40912286 0.52682017 0.         0.        ]\n",
      " [0.34261996 0.         0.         0.45050407 0.45050407 0.45050407\n",
      "  0.26607496 0.         0.45050407 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#第二种方法：直接使用feature_extraction.text模块的TfidfVectorizer类\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#直接用分词后得到的列表计算TF-IDF特征表示\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "tfidf_fit=tfidf.fit_transform(split_corpus)\n",
    "print(tfidf_fit.toarray()) #显示TF-IDF特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 垃圾邮件识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 构建文本分类特征训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#实例 垃圾邮件识别\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#构造文本分类特征集\n",
    "train_file = open(\"data/mailcorpus.txt\", 'r', encoding = \"utf-8\")\n",
    "corpus = train_file.readlines()   #列表中的每个元素为一行文本\n",
    "#分词\n",
    "split_corpus = []\n",
    "for c in corpus:\n",
    "    split_corpus.append(\" \".join(jieba.lcut(c)))\n",
    "    \n",
    "#使用词袋模型提取特征，得到文本特征矩阵\n",
    "cv = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = cv.fit_transform(split_corpus).toarray()\n",
    "\n",
    "#构造标签向量，垃圾标签为0，正常标签为1\n",
    "y = [0] * 5000 + [1] * 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 模型训练与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将特征集 分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_bayes accuracy:\n",
      " 0.9865\n",
      "naive_bayes report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1993\n",
      "           1       0.98      1.00      0.99      2007\n",
      "\n",
      "    accuracy                           0.99      4000\n",
      "   macro avg       0.99      0.99      0.99      4000\n",
      "weighted avg       0.99      0.99      0.99      4000\n",
      "\n",
      "naive_bayes matrix:\n",
      " [[1948   45]\n",
      " [   9 1998]]\n"
     ]
    }
   ],
   "source": [
    "#使用朴素贝叶斯训练分类模型，给出分类效果\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test) \n",
    "\n",
    "#朴素贝叶斯模型分类效果\n",
    "print(\"naive_bayes accuracy:\\n\",gnb.score(X_test, y_test))\n",
    "print(\"naive_bayes report:\\n\",metrics.classification_report(y_test, y_pred))\n",
    "print(\"naive_bayes matrix:\\n\",metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy:\n",
      " 0.92525\n",
      "SVM report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      1993\n",
      "           1       0.87      1.00      0.93      2007\n",
      "\n",
      "    accuracy                           0.93      4000\n",
      "   macro avg       0.93      0.92      0.92      4000\n",
      "weighted avg       0.93      0.93      0.92      4000\n",
      "\n",
      "SVM matrix:\n",
      " [[1696  297]\n",
      " [   2 2005]]\n"
     ]
    }
   ],
   "source": [
    "#使用SVM训练分类模型\n",
    "svm = svm.SVC(kernel='rbf', gamma=0.7, C = 1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#SVM分类性能\n",
    "y_pred_svm = svm.predict(X_test)   \n",
    "\n",
    "print(\"SVM accuracy:\\n\",svm.score(X_test, y_test))\n",
    "print(\"SVM report:\\n\",metrics.classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM matrix:\\n\",metrics.confusion_matrix(y_test, y_pred_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
